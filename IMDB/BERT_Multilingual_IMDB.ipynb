{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a99941b-8655-4080-8c4b-68c9e94a7011",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr  4 12:22:39 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1080        On  | 00000000:04:00.0 Off |                  N/A |\n",
      "|  0%   26C    P8               6W / 180W |    666MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce GTX 1080        On  | 00000000:05:00.0 Off |                  N/A |\n",
      "|  0%   26C    P8               6W / 180W |    666MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 2080 Ti     On  | 00000000:08:00.0 Off |                  N/A |\n",
      "|  0%   25C    P8              15W / 250W |   1083MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 2080 Ti     On  | 00000000:09:00.0 Off |                  N/A |\n",
      "|  0%   24C    P8               1W / 250W |    941MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce GTX 1080        On  | 00000000:83:00.0 Off |                  N/A |\n",
      "|  0%   27C    P8               6W / 180W |    666MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce GTX 1080        On  | 00000000:84:00.0 Off |                  N/A |\n",
      "|  0%   26C    P8               6W / 180W |    666MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA GeForce RTX 2080 Ti     On  | 00000000:87:00.0 Off |                  N/A |\n",
      "|  0%   25C    P8              17W / 250W |    941MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A    122596      C   /usr/bin/python3                            220MiB |\n",
      "|    0   N/A  N/A   1485400      C   /usr/bin/python3                            220MiB |\n",
      "|    0   N/A  N/A   1487832      C   /usr/bin/python3                            220MiB |\n",
      "|    1   N/A  N/A    122596      C   /usr/bin/python3                            220MiB |\n",
      "|    1   N/A  N/A   1485400      C   /usr/bin/python3                            220MiB |\n",
      "|    1   N/A  N/A   1487832      C   /usr/bin/python3                            220MiB |\n",
      "|    2   N/A  N/A    122596      C   /usr/bin/python3                            192MiB |\n",
      "|    2   N/A  N/A   1485400      C   /usr/bin/python3                            218MiB |\n",
      "|    2   N/A  N/A   1487832      C   /usr/bin/python3                            196MiB |\n",
      "|    2   N/A  N/A   1537672      C   /usr/bin/python3                            472MiB |\n",
      "|    3   N/A  N/A    122596      C   /usr/bin/python3                            312MiB |\n",
      "|    3   N/A  N/A   1485400      C   /usr/bin/python3                            312MiB |\n",
      "|    3   N/A  N/A   1487832      C   /usr/bin/python3                            312MiB |\n",
      "|    4   N/A  N/A    122596      C   /usr/bin/python3                            220MiB |\n",
      "|    4   N/A  N/A   1485400      C   /usr/bin/python3                            220MiB |\n",
      "|    4   N/A  N/A   1487832      C   /usr/bin/python3                            220MiB |\n",
      "|    5   N/A  N/A    122596      C   /usr/bin/python3                            220MiB |\n",
      "|    5   N/A  N/A   1485400      C   /usr/bin/python3                            220MiB |\n",
      "|    5   N/A  N/A   1487832      C   /usr/bin/python3                            220MiB |\n",
      "|    6   N/A  N/A    122596      C   /usr/bin/python3                            312MiB |\n",
      "|    6   N/A  N/A   1485400      C   /usr/bin/python3                            312MiB |\n",
      "|    6   N/A  N/A   1487832      C   /usr/bin/python3                            312MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3df45e9a-e792-4a95-879e-c419ae298f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the CUDA_VISIBLE_DEVICES environment variable to the index of the GPU you want to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61eeb061-e324-44ac-ad34-980b773f1a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hygumm/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "dataset = load_dataset(\"stanfordnlp/imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b66f490-95c1-43f5-a60c-1a9001b600e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = dataset[\"train\"].to_pandas()\n",
    "test_df = dataset[\"test\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e36cc8af-8047-4a14-9857-bf0e26f8958b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame:\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "print(\"Merged DataFrame:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fec17014-0566-4e32-aff4-fd81c9f4a411",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled DataFrame:\n",
      "                                                    text  label\n",
      "0      Forget what I said about Emeril. Rachael Ray i...      0\n",
      "1      Former private eye-turned-security guard ditch...      0\n",
      "2      Mann photographs the Alberta Rocky Mountains i...      0\n",
      "3      Simply put: the movie is boring. Clich√© upon c...      0\n",
      "4      Now being a fan of sci fi, the trailer for thi...      1\n",
      "...                                                  ...    ...\n",
      "49995  The \"documentary\", and we use that term loosel...      0\n",
      "49996  This outlandish Troma movie is actually a very...      1\n",
      "49997  I found the film Don't Look In The Basement to...      1\n",
      "49998  I have read the novel Reaper of Ben Mezrich a ...      0\n",
      "49999  Went to see this finnish film and I've got to ...      1\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming merged_df is already defined\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display the shuffled DataFrame\n",
    "print(\"Shuffled DataFrame:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2128b7ff-8e6d-4227-b0a6-30e6d683d2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('imdb.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46c6aed9-2770-4c8b-a1ff-81745e5bd866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08b3f3a5-0de6-46b0-b5d7-72f45808953f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# df = pd.read_csv(\"file1.csv\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ea141e8-73e4-4df4-bff0-84bd1f9938d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.rename(columns = {'review':'text'}, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a16f3-49cb-4f31-af58-18851168422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop('Unnamed: 0', inplace= True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aae243-3b52-439c-9b6a-adc0bc2d1547",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.sample(n=1000, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90ded249-8988-4dee-aaea-ceb48b0d25dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c74616b9-6b0f-4979-b7e6-2f32201568d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "616c9b34-5130-4d06-b881-64decf1631ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "def preprocess_data(df, tokenizer, max_len=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in df['text']:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(df['label'].values)\n",
    "\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "# Preprocess both the training and testing set\n",
    "train_inputs, train_masks, train_labels = preprocess_data(train_df, tokenizer)\n",
    "test_inputs, test_masks, test_labels = preprocess_data(test_df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dc03d7a-c6b1-47dd-821c-46d14223003f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "347efca9-eab0-491c-a0c7-5d84ded64915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 16  # Adjust based on your hardware capabilities\n",
    "\n",
    "# Create the DataLoader for the training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for the test set\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6627fa94-bbd1-4e73-b0c7-bc1a42747914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Load the BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-multilingual-cased',\n",
    "    num_labels=2,  # Binary classification\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "# Specify the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1898dcff-3c09-4464-86c0-fd1d683f3ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63d37587-0b08-44f1-b6cb-ea1733fe3534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-04 12:29:01.180457: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-04 12:29:01.492131: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-04 12:29:02.752391: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-04 12:29:02.752429: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/hygumm/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=2e-5,  # Learning rate\n",
    "                  eps=1e-8)  # Epsilon to prevent division by zero in optimizer\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is the number of batches * number of epochs\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0,  # Default value\n",
    "                                            num_training_steps=total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24251b3-135a-4f9f-be4a-74828256a8fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26f25dfb-dc96-4156-9a37-20d83f378d84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  2,500.    Elapsed: 0:00:13.\n",
      "  Batch    80  of  2,500.    Elapsed: 0:00:25.\n",
      "  Batch   120  of  2,500.    Elapsed: 0:00:38.\n",
      "  Batch   160  of  2,500.    Elapsed: 0:00:50.\n",
      "  Batch   200  of  2,500.    Elapsed: 0:01:02.\n",
      "  Batch   240  of  2,500.    Elapsed: 0:01:15.\n",
      "  Batch   280  of  2,500.    Elapsed: 0:01:27.\n",
      "  Batch   320  of  2,500.    Elapsed: 0:01:39.\n",
      "  Batch   360  of  2,500.    Elapsed: 0:01:51.\n",
      "  Batch   400  of  2,500.    Elapsed: 0:02:04.\n",
      "  Batch   440  of  2,500.    Elapsed: 0:02:16.\n",
      "  Batch   480  of  2,500.    Elapsed: 0:02:28.\n",
      "  Batch   520  of  2,500.    Elapsed: 0:02:41.\n",
      "  Batch   560  of  2,500.    Elapsed: 0:02:53.\n",
      "  Batch   600  of  2,500.    Elapsed: 0:03:05.\n",
      "  Batch   640  of  2,500.    Elapsed: 0:03:18.\n",
      "  Batch   680  of  2,500.    Elapsed: 0:03:30.\n",
      "  Batch   720  of  2,500.    Elapsed: 0:03:42.\n",
      "  Batch   760  of  2,500.    Elapsed: 0:03:55.\n",
      "  Batch   800  of  2,500.    Elapsed: 0:04:07.\n",
      "  Batch   840  of  2,500.    Elapsed: 0:04:20.\n",
      "  Batch   880  of  2,500.    Elapsed: 0:04:32.\n",
      "  Batch   920  of  2,500.    Elapsed: 0:04:44.\n",
      "  Batch   960  of  2,500.    Elapsed: 0:04:57.\n",
      "  Batch 1,000  of  2,500.    Elapsed: 0:05:09.\n",
      "  Batch 1,040  of  2,500.    Elapsed: 0:05:21.\n",
      "  Batch 1,080  of  2,500.    Elapsed: 0:05:34.\n",
      "  Batch 1,120  of  2,500.    Elapsed: 0:05:46.\n",
      "  Batch 1,160  of  2,500.    Elapsed: 0:05:58.\n",
      "  Batch 1,200  of  2,500.    Elapsed: 0:06:11.\n",
      "  Batch 1,240  of  2,500.    Elapsed: 0:06:23.\n",
      "  Batch 1,280  of  2,500.    Elapsed: 0:06:36.\n",
      "  Batch 1,320  of  2,500.    Elapsed: 0:06:48.\n",
      "  Batch 1,360  of  2,500.    Elapsed: 0:07:00.\n",
      "  Batch 1,400  of  2,500.    Elapsed: 0:07:13.\n",
      "  Batch 1,440  of  2,500.    Elapsed: 0:07:25.\n",
      "  Batch 1,480  of  2,500.    Elapsed: 0:07:37.\n",
      "  Batch 1,520  of  2,500.    Elapsed: 0:07:50.\n",
      "  Batch 1,560  of  2,500.    Elapsed: 0:08:02.\n",
      "  Batch 1,600  of  2,500.    Elapsed: 0:08:15.\n",
      "  Batch 1,640  of  2,500.    Elapsed: 0:08:27.\n",
      "  Batch 1,680  of  2,500.    Elapsed: 0:08:39.\n",
      "  Batch 1,720  of  2,500.    Elapsed: 0:08:52.\n",
      "  Batch 1,760  of  2,500.    Elapsed: 0:09:04.\n",
      "  Batch 1,800  of  2,500.    Elapsed: 0:09:16.\n",
      "  Batch 1,840  of  2,500.    Elapsed: 0:09:29.\n",
      "  Batch 1,880  of  2,500.    Elapsed: 0:09:41.\n",
      "  Batch 1,920  of  2,500.    Elapsed: 0:09:54.\n",
      "  Batch 1,960  of  2,500.    Elapsed: 0:10:06.\n",
      "  Batch 2,000  of  2,500.    Elapsed: 0:10:18.\n",
      "  Batch 2,040  of  2,500.    Elapsed: 0:10:31.\n",
      "  Batch 2,080  of  2,500.    Elapsed: 0:10:43.\n",
      "  Batch 2,120  of  2,500.    Elapsed: 0:10:56.\n",
      "  Batch 2,160  of  2,500.    Elapsed: 0:11:08.\n",
      "  Batch 2,200  of  2,500.    Elapsed: 0:11:20.\n",
      "  Batch 2,240  of  2,500.    Elapsed: 0:11:33.\n",
      "  Batch 2,280  of  2,500.    Elapsed: 0:11:45.\n",
      "  Batch 2,320  of  2,500.    Elapsed: 0:11:57.\n",
      "  Batch 2,360  of  2,500.    Elapsed: 0:12:10.\n",
      "  Batch 2,400  of  2,500.    Elapsed: 0:12:22.\n",
      "  Batch 2,440  of  2,500.    Elapsed: 0:12:35.\n",
      "  Batch 2,480  of  2,500.    Elapsed: 0:12:47.\n",
      "\n",
      "  Average training loss: 0.40\n",
      "  Training epoch took: 0:12:53\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.85\n",
      "  Validation Loss: 0.35\n",
      "  Validation took: 0:01:06\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  2,500.    Elapsed: 0:00:12.\n",
      "  Batch    80  of  2,500.    Elapsed: 0:00:25.\n",
      "  Batch   120  of  2,500.    Elapsed: 0:00:37.\n",
      "  Batch   160  of  2,500.    Elapsed: 0:00:50.\n",
      "  Batch   200  of  2,500.    Elapsed: 0:01:02.\n",
      "  Batch   240  of  2,500.    Elapsed: 0:01:14.\n",
      "  Batch   280  of  2,500.    Elapsed: 0:01:27.\n",
      "  Batch   320  of  2,500.    Elapsed: 0:01:39.\n",
      "  Batch   360  of  2,500.    Elapsed: 0:01:51.\n",
      "  Batch   400  of  2,500.    Elapsed: 0:02:04.\n",
      "  Batch   440  of  2,500.    Elapsed: 0:02:16.\n",
      "  Batch   480  of  2,500.    Elapsed: 0:02:29.\n",
      "  Batch   520  of  2,500.    Elapsed: 0:02:41.\n",
      "  Batch   560  of  2,500.    Elapsed: 0:02:53.\n",
      "  Batch   600  of  2,500.    Elapsed: 0:03:06.\n",
      "  Batch   640  of  2,500.    Elapsed: 0:03:18.\n",
      "  Batch   680  of  2,500.    Elapsed: 0:03:31.\n",
      "  Batch   720  of  2,500.    Elapsed: 0:03:43.\n",
      "  Batch   760  of  2,500.    Elapsed: 0:03:55.\n",
      "  Batch   800  of  2,500.    Elapsed: 0:04:08.\n",
      "  Batch   840  of  2,500.    Elapsed: 0:04:20.\n",
      "  Batch   880  of  2,500.    Elapsed: 0:04:33.\n",
      "  Batch   920  of  2,500.    Elapsed: 0:04:45.\n",
      "  Batch   960  of  2,500.    Elapsed: 0:04:57.\n",
      "  Batch 1,000  of  2,500.    Elapsed: 0:05:10.\n",
      "  Batch 1,040  of  2,500.    Elapsed: 0:05:22.\n",
      "  Batch 1,080  of  2,500.    Elapsed: 0:05:35.\n",
      "  Batch 1,120  of  2,500.    Elapsed: 0:05:47.\n",
      "  Batch 1,160  of  2,500.    Elapsed: 0:05:59.\n",
      "  Batch 1,200  of  2,500.    Elapsed: 0:06:12.\n",
      "  Batch 1,240  of  2,500.    Elapsed: 0:06:24.\n",
      "  Batch 1,280  of  2,500.    Elapsed: 0:06:36.\n",
      "  Batch 1,320  of  2,500.    Elapsed: 0:06:49.\n",
      "  Batch 1,360  of  2,500.    Elapsed: 0:07:01.\n",
      "  Batch 1,400  of  2,500.    Elapsed: 0:07:14.\n",
      "  Batch 1,440  of  2,500.    Elapsed: 0:07:26.\n",
      "  Batch 1,480  of  2,500.    Elapsed: 0:07:38.\n",
      "  Batch 1,520  of  2,500.    Elapsed: 0:07:51.\n",
      "  Batch 1,560  of  2,500.    Elapsed: 0:08:03.\n",
      "  Batch 1,600  of  2,500.    Elapsed: 0:08:16.\n",
      "  Batch 1,640  of  2,500.    Elapsed: 0:08:28.\n",
      "  Batch 1,680  of  2,500.    Elapsed: 0:08:40.\n",
      "  Batch 1,720  of  2,500.    Elapsed: 0:08:53.\n",
      "  Batch 1,760  of  2,500.    Elapsed: 0:09:05.\n",
      "  Batch 1,800  of  2,500.    Elapsed: 0:09:17.\n",
      "  Batch 1,840  of  2,500.    Elapsed: 0:09:30.\n",
      "  Batch 1,880  of  2,500.    Elapsed: 0:09:42.\n",
      "  Batch 1,920  of  2,500.    Elapsed: 0:09:55.\n",
      "  Batch 1,960  of  2,500.    Elapsed: 0:10:07.\n",
      "  Batch 2,000  of  2,500.    Elapsed: 0:10:19.\n",
      "  Batch 2,040  of  2,500.    Elapsed: 0:10:32.\n",
      "  Batch 2,080  of  2,500.    Elapsed: 0:10:44.\n",
      "  Batch 2,120  of  2,500.    Elapsed: 0:10:57.\n",
      "  Batch 2,160  of  2,500.    Elapsed: 0:11:09.\n",
      "  Batch 2,200  of  2,500.    Elapsed: 0:11:21.\n",
      "  Batch 2,240  of  2,500.    Elapsed: 0:11:34.\n",
      "  Batch 2,280  of  2,500.    Elapsed: 0:11:46.\n",
      "  Batch 2,320  of  2,500.    Elapsed: 0:11:58.\n",
      "  Batch 2,360  of  2,500.    Elapsed: 0:12:11.\n",
      "  Batch 2,400  of  2,500.    Elapsed: 0:12:23.\n",
      "  Batch 2,440  of  2,500.    Elapsed: 0:12:36.\n",
      "  Batch 2,480  of  2,500.    Elapsed: 0:12:48.\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epoch took: 0:12:54\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.87\n",
      "  Validation Loss: 0.35\n",
      "  Validation took: 0:01:06\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  2,500.    Elapsed: 0:00:12.\n",
      "  Batch    80  of  2,500.    Elapsed: 0:00:25.\n",
      "  Batch   120  of  2,500.    Elapsed: 0:00:37.\n",
      "  Batch   160  of  2,500.    Elapsed: 0:00:50.\n",
      "  Batch   200  of  2,500.    Elapsed: 0:01:02.\n",
      "  Batch   240  of  2,500.    Elapsed: 0:01:14.\n",
      "  Batch   280  of  2,500.    Elapsed: 0:01:27.\n",
      "  Batch   320  of  2,500.    Elapsed: 0:01:39.\n",
      "  Batch   360  of  2,500.    Elapsed: 0:01:51.\n",
      "  Batch   400  of  2,500.    Elapsed: 0:02:04.\n",
      "  Batch   440  of  2,500.    Elapsed: 0:02:16.\n",
      "  Batch   480  of  2,500.    Elapsed: 0:02:29.\n",
      "  Batch   520  of  2,500.    Elapsed: 0:02:41.\n",
      "  Batch   560  of  2,500.    Elapsed: 0:02:53.\n",
      "  Batch   600  of  2,500.    Elapsed: 0:03:06.\n",
      "  Batch   640  of  2,500.    Elapsed: 0:03:18.\n",
      "  Batch   680  of  2,500.    Elapsed: 0:03:31.\n",
      "  Batch   720  of  2,500.    Elapsed: 0:03:43.\n",
      "  Batch   760  of  2,500.    Elapsed: 0:03:55.\n",
      "  Batch   800  of  2,500.    Elapsed: 0:04:08.\n",
      "  Batch   840  of  2,500.    Elapsed: 0:04:20.\n",
      "  Batch   880  of  2,500.    Elapsed: 0:04:32.\n",
      "  Batch   920  of  2,500.    Elapsed: 0:04:45.\n",
      "  Batch   960  of  2,500.    Elapsed: 0:04:57.\n",
      "  Batch 1,000  of  2,500.    Elapsed: 0:05:10.\n",
      "  Batch 1,040  of  2,500.    Elapsed: 0:05:22.\n",
      "  Batch 1,080  of  2,500.    Elapsed: 0:05:34.\n",
      "  Batch 1,120  of  2,500.    Elapsed: 0:05:47.\n",
      "  Batch 1,160  of  2,500.    Elapsed: 0:05:59.\n",
      "  Batch 1,200  of  2,500.    Elapsed: 0:06:12.\n",
      "  Batch 1,240  of  2,500.    Elapsed: 0:06:24.\n",
      "  Batch 1,280  of  2,500.    Elapsed: 0:06:36.\n",
      "  Batch 1,320  of  2,500.    Elapsed: 0:06:49.\n",
      "  Batch 1,360  of  2,500.    Elapsed: 0:07:01.\n",
      "  Batch 1,400  of  2,500.    Elapsed: 0:07:14.\n",
      "  Batch 1,440  of  2,500.    Elapsed: 0:07:26.\n",
      "  Batch 1,480  of  2,500.    Elapsed: 0:07:38.\n",
      "  Batch 1,520  of  2,500.    Elapsed: 0:07:51.\n",
      "  Batch 1,560  of  2,500.    Elapsed: 0:08:03.\n",
      "  Batch 1,600  of  2,500.    Elapsed: 0:08:16.\n",
      "  Batch 1,640  of  2,500.    Elapsed: 0:08:28.\n",
      "  Batch 1,680  of  2,500.    Elapsed: 0:08:40.\n",
      "  Batch 1,720  of  2,500.    Elapsed: 0:08:53.\n",
      "  Batch 1,760  of  2,500.    Elapsed: 0:09:05.\n",
      "  Batch 1,800  of  2,500.    Elapsed: 0:09:18.\n",
      "  Batch 1,840  of  2,500.    Elapsed: 0:09:30.\n",
      "  Batch 1,880  of  2,500.    Elapsed: 0:09:42.\n",
      "  Batch 1,920  of  2,500.    Elapsed: 0:09:55.\n",
      "  Batch 1,960  of  2,500.    Elapsed: 0:10:07.\n",
      "  Batch 2,000  of  2,500.    Elapsed: 0:10:20.\n",
      "  Batch 2,040  of  2,500.    Elapsed: 0:10:32.\n",
      "  Batch 2,080  of  2,500.    Elapsed: 0:10:44.\n",
      "  Batch 2,120  of  2,500.    Elapsed: 0:10:57.\n",
      "  Batch 2,160  of  2,500.    Elapsed: 0:11:09.\n",
      "  Batch 2,200  of  2,500.    Elapsed: 0:11:22.\n",
      "  Batch 2,240  of  2,500.    Elapsed: 0:11:34.\n",
      "  Batch 2,280  of  2,500.    Elapsed: 0:11:46.\n",
      "  Batch 2,320  of  2,500.    Elapsed: 0:11:59.\n",
      "  Batch 2,360  of  2,500.    Elapsed: 0:12:11.\n",
      "  Batch 2,400  of  2,500.    Elapsed: 0:12:24.\n",
      "  Batch 2,440  of  2,500.    Elapsed: 0:12:36.\n",
      "  Batch 2,480  of  2,500.    Elapsed: 0:12:48.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:12:55\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.87\n",
      "  Validation Loss: 0.44\n",
      "  Validation took: 0:01:06\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  2,500.    Elapsed: 0:00:12.\n",
      "  Batch    80  of  2,500.    Elapsed: 0:00:25.\n",
      "  Batch   120  of  2,500.    Elapsed: 0:00:37.\n",
      "  Batch   160  of  2,500.    Elapsed: 0:00:50.\n",
      "  Batch   200  of  2,500.    Elapsed: 0:01:02.\n",
      "  Batch   240  of  2,500.    Elapsed: 0:01:14.\n",
      "  Batch   280  of  2,500.    Elapsed: 0:01:27.\n",
      "  Batch   320  of  2,500.    Elapsed: 0:01:39.\n",
      "  Batch   360  of  2,500.    Elapsed: 0:01:52.\n",
      "  Batch   400  of  2,500.    Elapsed: 0:02:04.\n",
      "  Batch   440  of  2,500.    Elapsed: 0:02:16.\n",
      "  Batch   480  of  2,500.    Elapsed: 0:02:29.\n",
      "  Batch   520  of  2,500.    Elapsed: 0:02:41.\n",
      "  Batch   560  of  2,500.    Elapsed: 0:02:54.\n",
      "  Batch   600  of  2,500.    Elapsed: 0:03:06.\n",
      "  Batch   640  of  2,500.    Elapsed: 0:03:18.\n",
      "  Batch   680  of  2,500.    Elapsed: 0:03:31.\n",
      "  Batch   720  of  2,500.    Elapsed: 0:03:43.\n",
      "  Batch   760  of  2,500.    Elapsed: 0:03:56.\n",
      "  Batch   800  of  2,500.    Elapsed: 0:04:08.\n",
      "  Batch   840  of  2,500.    Elapsed: 0:04:20.\n",
      "  Batch   880  of  2,500.    Elapsed: 0:04:33.\n",
      "  Batch   920  of  2,500.    Elapsed: 0:04:45.\n",
      "  Batch   960  of  2,500.    Elapsed: 0:04:58.\n",
      "  Batch 1,000  of  2,500.    Elapsed: 0:05:10.\n",
      "  Batch 1,040  of  2,500.    Elapsed: 0:05:22.\n",
      "  Batch 1,080  of  2,500.    Elapsed: 0:05:35.\n",
      "  Batch 1,120  of  2,500.    Elapsed: 0:05:47.\n",
      "  Batch 1,160  of  2,500.    Elapsed: 0:06:00.\n",
      "  Batch 1,200  of  2,500.    Elapsed: 0:06:12.\n",
      "  Batch 1,240  of  2,500.    Elapsed: 0:06:25.\n",
      "  Batch 1,280  of  2,500.    Elapsed: 0:06:37.\n",
      "  Batch 1,320  of  2,500.    Elapsed: 0:06:49.\n",
      "  Batch 1,360  of  2,500.    Elapsed: 0:07:02.\n",
      "  Batch 1,400  of  2,500.    Elapsed: 0:07:14.\n",
      "  Batch 1,440  of  2,500.    Elapsed: 0:07:27.\n",
      "  Batch 1,480  of  2,500.    Elapsed: 0:07:39.\n",
      "  Batch 1,520  of  2,500.    Elapsed: 0:07:51.\n",
      "  Batch 1,560  of  2,500.    Elapsed: 0:08:04.\n",
      "  Batch 1,600  of  2,500.    Elapsed: 0:08:16.\n",
      "  Batch 1,640  of  2,500.    Elapsed: 0:08:29.\n",
      "  Batch 1,680  of  2,500.    Elapsed: 0:08:41.\n",
      "  Batch 1,720  of  2,500.    Elapsed: 0:08:54.\n",
      "  Batch 1,760  of  2,500.    Elapsed: 0:09:06.\n",
      "  Batch 1,800  of  2,500.    Elapsed: 0:09:18.\n",
      "  Batch 1,840  of  2,500.    Elapsed: 0:09:31.\n",
      "  Batch 1,880  of  2,500.    Elapsed: 0:09:43.\n",
      "  Batch 1,920  of  2,500.    Elapsed: 0:09:56.\n",
      "  Batch 1,960  of  2,500.    Elapsed: 0:10:08.\n",
      "  Batch 2,000  of  2,500.    Elapsed: 0:10:20.\n",
      "  Batch 2,040  of  2,500.    Elapsed: 0:10:33.\n",
      "  Batch 2,080  of  2,500.    Elapsed: 0:10:45.\n",
      "  Batch 2,120  of  2,500.    Elapsed: 0:10:58.\n",
      "  Batch 2,160  of  2,500.    Elapsed: 0:11:10.\n",
      "  Batch 2,200  of  2,500.    Elapsed: 0:11:23.\n",
      "  Batch 2,240  of  2,500.    Elapsed: 0:11:35.\n",
      "  Batch 2,280  of  2,500.    Elapsed: 0:11:47.\n",
      "  Batch 2,320  of  2,500.    Elapsed: 0:12:00.\n",
      "  Batch 2,360  of  2,500.    Elapsed: 0:12:12.\n",
      "  Batch 2,400  of  2,500.    Elapsed: 0:12:25.\n",
      "  Batch 2,440  of  2,500.    Elapsed: 0:12:37.\n",
      "  Batch 2,480  of  2,500.    Elapsed: 0:12:49.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epoch took: 0:12:56\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.87\n",
      "  Validation Loss: 0.59\n",
      "  Validation took: 0:01:06\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Function to calculate the accuracy of predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# Function to format elapsed times as hh:mm:ss\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round(elapsed))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "# Training loop\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progress update every 40 batches\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a backward pass\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch)\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Accumulate the training loss over all of the batches\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(\"\\n  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on our validation set\n",
    "\n",
    "    print(\"\\nRunning Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "            \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(test_dataloader)\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "torch.save(model.state_dict(), 'final_bert_imdb.pt')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc2fdf61-754f-490d-af1e-eeae8b85a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, tokenizer):\n",
    "    model.eval()\n",
    "\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        None,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "    return predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "258d2671-b869-46b9-bb77-c108fd2deaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"I hate the  movie I did not like it at all\"\n",
    "prediction = predict_sentiment(text, model, tokenizer)\n",
    "print(\"Predicted sentiment:\", \"Positive\" if prediction == 1 else \"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20078803-bd54-45d9-89be-7b5390a2a54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = '''\"Aaaaand The Star Buoy hits it out of the park yet again!\n",
    "\n",
    "What a hilarious ride. Tillu is a true blue phenomenon in the realm of Telugu Cinema üíô \n",
    "And nobody can do justice to it like Siddu!\n",
    "What energy, what charm ‚ù§Ô∏è\n",
    "\n",
    "Tillu is not to be reviewed, questioned, or analyzed. He is simply meant to be loved, \n",
    "so gooo watch and enjoy the fun partyyy! The one-liners and Anupama(superbly written - stellar performance)\n",
    "are the other standouts in this Siddu Jonnalagadda  bonanza üéâ Don'tttt missss!\"'''\n",
    "prediction = predict_sentiment(text, model, tokenizer)\n",
    "print(\"Predicted sentiment:\", \"Positive\" if prediction == 1 else \"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c2eff26-7f45-4099-baf0-b6ae06158785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "text = \"One time watch .. good rom com movie but not up to the expectations‚Ä¶SIDDHU AS TILLU ROCKS again\"\n",
    "prediction = predict_sentiment(text, model, tokenizer)\n",
    "print(\"Predicted sentiment:\", \"Positive\" if prediction == 1 else \"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2d3020-e8cb-4c83-867a-ba92be779b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
