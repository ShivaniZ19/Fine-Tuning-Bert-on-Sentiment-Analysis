{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install torch -U\n",
        "!pip install plotly\n",
        "!pip install livelossplot\n",
        "!pip install imageio\n",
        "!pip install sacrebleu\n",
        "!pip install seaborn\n",
        "!pip install datasets\n",
        "!pip install accelerate -U\n",
        "!pip install transformers[torch]\n",
        "!pip install fasttext\n",
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "UnWY5nPCyZrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PhoBERT"
      ],
      "metadata": {
        "id": "gWJd37IyKfu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim import AdamW\n",
        "from tqdm.notebook import tqdm\n",
        "import seaborn as sns\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# Function to calculate class weights\n",
        "def calculate_class_weights(class_counts):\n",
        "    total = sum(class_counts)\n",
        "    weights = [total / class_count for class_count in class_counts]\n",
        "    return torch.tensor(weights, dtype=torch.float32)\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base-v2\", num_labels=2)\n",
        "\n",
        "# Load a small subset of training and testing data (10 rows each)\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/NLP/NTC_SV_train_1.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/NLP/NTC_SV_test_1.csv')\n",
        "\n",
        "# Tokenize the data with padding and create attention masks\n",
        "train_encodings = tokenizer(train_df['review'].tolist(), truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
        "test_encodings = tokenizer(test_df['review'].tolist(), truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
        "\n",
        "# Define a custom dataset\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "        # Pass the labels directly if they are already tensors\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Convert labels to a tensor\n",
        "train_labels = torch.tensor(train_df['label'].tolist())\n",
        "test_labels = torch.tensor(test_df['label'].tolist())\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = calculate_class_weights([sum(train_labels == 0), sum(train_labels == 1)])\n",
        "\n",
        "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
        "test_dataset = SentimentDataset(test_encodings, test_labels)\n",
        "\n",
        "# DataLoader\n",
        "def collate_fn(batch):\n",
        "    return {key: torch.stack([item[key] for item in batch]) for key in batch[0]}\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Loss function with class weights\n",
        "loss_function = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Initialize the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader)*3)\n",
        "\n",
        "# Metrics tracking\n",
        "epoch_metrics = {\n",
        "    'loss': [],\n",
        "    'accuracy': [],\n",
        "    'precision': [],\n",
        "    'recall': [],\n",
        "    'f1': []\n",
        "}\n",
        "\n",
        "# Train and evaluate the model\n",
        "model.train()\n",
        "for epoch in range(3):  # More epochs for better training\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_examples = 0\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "\n",
        "    for batch in loop:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Include attention mask in the model's forward pass\n",
        "        input_ids = batch['input_ids'].to(torch.long)\n",
        "        attention_mask = batch['attention_mask'].to(torch.long)\n",
        "        labels = batch['labels'].to(torch.long)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = loss_function(outputs.logits, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update the learning rate\n",
        "\n",
        "        # Calculate running loss and accuracy\n",
        "        running_loss += loss.item() * input_ids.size(0)\n",
        "        running_corrects += torch.sum(torch.argmax(outputs.logits, axis=1) == labels)\n",
        "        total_examples += input_ids.size(0)\n",
        "\n",
        "        loop.set_description(f'Epoch {epoch}')\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    epoch_loss = running_loss / total_examples\n",
        "    epoch_acc = running_corrects.double() / total_examples\n",
        "\n",
        "    epoch_metrics['loss'].append(epoch_loss)\n",
        "    epoch_metrics['accuracy'].append(epoch_acc.item())\n",
        "\n",
        "    # Evaluate model performance after each epoch\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(torch.long)\n",
        "            attention_mask = batch['attention_mask'].to(torch.long)\n",
        "            labels = batch['labels'].to(torch.long)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            predictions.extend(torch.argmax(logits, axis=1).tolist())\n",
        "            true_labels.extend(labels.tolist())\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, zero_division=0)\n",
        "    recall = recall_score(true_labels, predictions, zero_division=0)\n",
        "    f1 = f1_score(true_labels, predictions, zero_division=0)\n",
        "\n",
        "    epoch_metrics['precision'].append(precision)\n",
        "    epoch_metrics['recall'].append(recall)\n",
        "    epoch_metrics['f1'].append(f1)\n",
        "\n",
        "    print(f'Epoch {epoch} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}')\n",
        "\n",
        "# Plot confusion matrix\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Line chart for each metric over epochs\n",
        "plt.figure(figsize=(10, 5))\n",
        "for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
        "    plt.plot(range(1, 4), epoch_metrics[metric], label=metric.capitalize())\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Metric Scores Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Bar chart summarizing overall performance\n",
        "overall_metrics = {metric: sum(epoch_metrics[metric]) / len(epoch_metrics[metric]) for metric in epoch_metrics}\n",
        "plt.bar(overall_metrics.keys(), overall_metrics.values(), color=['blue', 'green', 'red', 'purple', 'orange'])\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Values')\n",
        "plt.title('Overall Model Performance')\n",
        "plt.ylim([0, 1])\n",
        "plt.show()\n",
        "\n",
        "# Markdown table for performance metrics per epoch\n",
        "markdown_table = \"| Epoch | Loss | Accuracy | Precision | Recall | F1 |\\n\"\n",
        "markdown_table += \"|-------|------|----------|-----------|--------|----|\\n\"\n",
        "for i in range(3):\n",
        "    markdown_table += f\"| {i + 1} \"\n",
        "    markdown_table += \"|\".join(f\"{epoch_metrics[metric][i]:.4f}\" for metric in ['loss', 'accuracy', 'precision', 'recall', 'f1'])\n",
        "    markdown_table += \"|\\n\"\n",
        "\n",
        "# Last row for overall metrics\n",
        "markdown_table += \"| Overall \"\n",
        "markdown_table += \"|\".join(f\"{overall:.4f}\" for overall in overall_metrics.values())\n",
        "markdown_table += \"|\\n\"\n",
        "\n",
        "display(Markdown(markdown_table))\n"
      ],
      "metadata": {
        "id": "SG0-EtvrKezv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PhoBERT + FastText"
      ],
      "metadata": {
        "id": "RQlFKnWKeOsd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_huE-H2eNwu"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim import AdamW\n",
        "from tqdm.notebook import tqdm\n",
        "import seaborn as sns\n",
        "from IPython.display import Markdown\n",
        "import fasttext\n",
        "from huggingface_hub import hf_hub_download\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Load a small subset of training and testing data (10 rows each)\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/NLP/NTC_SV_train_1.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/NLP/NTC_SV_test_1.csv')\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base-v2\", num_labels=2)\n",
        "\n",
        "# Load the fastText Vietnamese model\n",
        "model_path = hf_hub_download(repo_id=\"facebook/fasttext-vi-vectors\", filename=\"model.bin\")\n",
        "ft_model = fasttext.load_model(model_path)\n",
        "\n",
        "# Function to calculate class weights\n",
        "def calculate_class_weights(class_counts):\n",
        "    y_train = train_df['label'].tolist()\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "    class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
        "    return class_weights\n",
        "\n",
        "# Tokenize the data with padding and create attention masks\n",
        "train_encodings = tokenizer(train_df['review'].tolist(), truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
        "test_encodings = tokenizer(test_df['review'].tolist(), truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
        "\n",
        "# Define a custom dataset\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_labels = torch.tensor(train_df['label'].tolist())\n",
        "test_labels = torch.tensor(test_df['label'].tolist())\n",
        "\n",
        "# Calculate class weights before they are used\n",
        "class_weights = calculate_class_weights([len(train_df[train_df['label'] == 0]), len(train_df[train_df['label'] == 1])])\n",
        "\n",
        "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
        "test_dataset = SentimentDataset(test_encodings, test_labels)\n",
        "\n",
        "# DataLoader\n",
        "def collate_fn(batch):\n",
        "    return {key: torch.stack([item[key] for item in batch]) for key in batch[0]}\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Loss function with class weights\n",
        "loss_function = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Initialize the optimizer with a more conservative learning rate\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * 3)\n",
        "\n",
        "# Metrics tracking\n",
        "epoch_metrics = {\n",
        "    'loss': [],\n",
        "    'accuracy': [],\n",
        "    'precision': [],\n",
        "    'recall': [],\n",
        "    'f1': []\n",
        "}\n",
        "\n",
        "# Training and evaluation loop\n",
        "best_loss = float('inf')\n",
        "best_model_state = None\n",
        "\n",
        "model.train()\n",
        "for epoch in range(3):  # More epochs for better training\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_examples = 0\n",
        "    predictions, true_labels = [], []  # Initialize the lists here\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "\n",
        "    for batch in loop:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(torch.long)\n",
        "        attention_mask = batch['attention_mask'].to(torch.long)\n",
        "        labels = batch['labels'].to(torch.long)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = loss_function(outputs.logits, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        running_loss += loss.item() * input_ids.size(0)\n",
        "        running_corrects += torch.sum(torch.argmax(outputs.logits, axis=1) == labels)\n",
        "        total_examples += input_ids.size(0)\n",
        "\n",
        "        loop.set_description(f'Epoch {epoch}')\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    epoch_loss = running_loss / total_examples\n",
        "    epoch_acc = running_corrects.double() / total_examples\n",
        "\n",
        "    epoch_metrics['loss'].append(epoch_loss)\n",
        "    epoch_metrics['accuracy'].append(epoch_acc.item())\n",
        "\n",
        "    # Evaluate model performance after each epoch\n",
        "    model.eval()\n",
        "    eval_loss = 0.0\n",
        "    eval_steps = 0\n",
        "    for batch in test_loader:\n",
        "        with torch.no_grad():\n",
        "            input_ids = batch['input_ids'].to(torch.long)\n",
        "            attention_mask = batch['attention_mask'].to(torch.long)\n",
        "            labels = batch['labels'].to(torch.long)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            logits = outputs.logits\n",
        "            tmp_eval_loss = loss_function(logits, labels)\n",
        "            eval_loss += tmp_eval_loss.mean().item()\n",
        "            eval_steps += 1\n",
        "            predictions.extend(torch.argmax(logits, axis=1).tolist())\n",
        "            true_labels.extend(labels.tolist())\n",
        "\n",
        "    # Calculate and print the average evaluation loss\n",
        "    avg_eval_loss = eval_loss / eval_steps\n",
        "    print(f\"Validation loss: {avg_eval_loss}\")\n",
        "\n",
        "    # Save the best model state based on validation loss\n",
        "    if avg_eval_loss < best_loss:\n",
        "        best_loss = avg_eval_loss\n",
        "        best_model_state = model.state_dict()\n",
        "\n",
        "    # Calculate the evaluation metrics using the validation predictions and true labels\n",
        "    val_accuracy = accuracy_score(true_labels, predictions)\n",
        "    val_precision = precision_score(true_labels, predictions, zero_division=0)\n",
        "    val_recall = recall_score(true_labels, predictions, zero_division=0)\n",
        "    val_f1 = f1_score(true_labels, predictions, zero_division=0)\n",
        "\n",
        "    epoch_metrics['precision'].append(val_precision)\n",
        "    epoch_metrics['recall'].append(val_recall)\n",
        "    epoch_metrics['f1'].append(val_f1)\n",
        "\n",
        "    print(f'Epoch {epoch} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}')\n",
        "\n",
        "# Load the best model state\n",
        "model.load_state_dict(best_model_state)\n",
        "\n",
        "# Plot confusion matrix\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Line chart for each metric over epochs\n",
        "plt.figure(figsize=(10, 5))\n",
        "for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
        "    plt.plot(range(1, 4), epoch_metrics[metric], label=metric.capitalize())\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Metric Scores Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Bar chart summarizing overall performance\n",
        "overall_metrics = {metric: sum(epoch_metrics[metric]) / len(epoch_metrics[metric]) for metric in epoch_metrics}\n",
        "plt.bar(overall_metrics.keys(), overall_metrics.values(), color=['blue', 'green', 'red', 'purple', 'orange'])\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Values')\n",
        "plt.title('Overall Model Performance')\n",
        "plt.ylim([0, 1])\n",
        "plt.show()\n",
        "\n",
        "# Markdown table for performance metrics per epoch\n",
        "markdown_table = \"| Epoch | Loss | Accuracy | Precision | Recall | F1 |\\n\"\n",
        "markdown_table += \"|-------|------|----------|-----------|--------|----|\\n\"\n",
        "for i in range(3):\n",
        "    markdown_table += f\"| {i + 1} \"\n",
        "    markdown_table += \"|\".join(f\"{epoch_metrics[metric][i]:.4f}\" for metric in ['loss', 'accuracy', 'precision', 'recall', 'f1'])\n",
        "    markdown_table += \"|\\n\"\n",
        "\n",
        "# Last row for overall metrics\n",
        "markdown_table += \"| Overall \"\n",
        "markdown_table += \"|\".join(f\"{overall:.4f}\" for overall in overall_metrics.values())\n",
        "markdown_table += \"|\\n\"\n",
        "\n",
        "display(Markdown(markdown_table))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PhoBERT + LSTM"
      ],
      "metadata": {
        "id": "0s3W68hKKzFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup, AutoModelForSequenceClassification  # Notice the change here to AutoModel\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim import AdamW\n",
        "from tqdm.notebook import tqdm\n",
        "import seaborn as sns\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# Function to calculate class weights\n",
        "def calculate_class_weights(class_counts):\n",
        "    total = sum(class_counts)\n",
        "    weights = [total / class_count for class_count in class_counts]\n",
        "    return torch.tensor(weights, dtype=torch.float32)\n",
        "\n",
        "# Load the tokenizer and base model without the classification head\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n",
        "#base_model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base-v2\", num_labels=2)\n",
        "base_model = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")  # Changed from AutoModelForSequenceClassification to AutoModel\n",
        "\n",
        "\n",
        "# Load a small subset of training and testing data (10 rows each)\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/NLP/NTC_SV_train_1.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/NLP/NTC_SV_test_1.csv')\n",
        "\n",
        "# Tokenize the data with padding and create attention masks\n",
        "train_encodings = tokenizer(train_df['review'].tolist(), truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
        "test_encodings = tokenizer(test_df['review'].tolist(), truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
        "\n",
        "# Define a custom dataset\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "        # Pass the labels directly if they are already tensors\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Convert labels to a tensor\n",
        "train_labels = torch.tensor(train_df['label'].tolist())\n",
        "test_labels = torch.tensor(test_df['label'].tolist())\n",
        "\n",
        "# Convert labels to tensors\n",
        "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
        "test_dataset = SentimentDataset(test_encodings, test_labels)\n",
        "\n",
        "# DataLoader\n",
        "def collate_fn(batch):\n",
        "    return {key: torch.stack([item[key] for item in batch]) for key in batch[0]}\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Add LSTM layer\n",
        "class LSTMClassifier(torch.nn.Module):\n",
        "    def __init__(self, base_model, hidden_dim, num_labels, bidirectional=True, dropout=0.1):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        self.lstm = torch.nn.LSTM(\n",
        "            input_size=self.base_model.config.hidden_size,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.fc = torch.nn.Linear(lstm_output_dim, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        lstm_out, _ = self.lstm(sequence_output)\n",
        "        logits = self.fc(lstm_out[:, -1, :])  # Using the output of the last timestep\n",
        "        return logits\n",
        "\n",
        "# Instantiate the model with LSTM\n",
        "lstm_model = LSTMClassifier(base_model, hidden_dim=128, num_labels=2)\n",
        "\n",
        "# Loss function with class weights\n",
        "class_weights = calculate_class_weights([sum(train_labels == 0), sum(train_labels == 1)])\n",
        "loss_function = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Initialize the optimizer\n",
        "optimizer = AdamW(lstm_model.parameters(), lr=2e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader)*3)\n",
        "\n",
        "# Metrics tracking\n",
        "epoch_metrics = {\n",
        "    'loss': [],\n",
        "    'accuracy': [],\n",
        "    'precision': [],\n",
        "    'recall': [],\n",
        "    'f1': []\n",
        "}\n",
        "\n",
        "# Train and evaluate the model\n",
        "lstm_model.train()\n",
        "for epoch in range(3):  # More epochs for better training\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_examples = 0\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "\n",
        "    for batch in loop:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Include attention mask in the model's forward pass\n",
        "        input_ids = batch['input_ids'].to(torch.long)\n",
        "        attention_mask = batch['attention_mask'].to(torch.long)\n",
        "        labels = batch['labels'].to(torch.long)\n",
        "\n",
        "        logits = lstm_model(input_ids, attention_mask)\n",
        "        loss = loss_function(logits, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(lstm_model.parameters(), max_norm=1.0)  # Gradient clipping\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update the learning rate\n",
        "\n",
        "        # Calculate running loss and accuracy\n",
        "        running_loss += loss.item() * input_ids.size(0)\n",
        "        running_corrects += torch.sum(torch.argmax(logits, axis=1) == labels)\n",
        "        total_examples += input_ids.size(0)\n",
        "\n",
        "        loop.set_description(f'Epoch {epoch}')\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    epoch_loss = running_loss / total_examples\n",
        "    epoch_acc = running_corrects.double() / total_examples\n",
        "\n",
        "    epoch_metrics['loss'].append(epoch_loss)\n",
        "    epoch_metrics['accuracy'].append(epoch_acc.item())\n",
        "\n",
        "    # Evaluate model performance after each epoch\n",
        "    lstm_model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(torch.long)\n",
        "            attention_mask = batch['attention_mask'].to(torch.long)\n",
        "            labels = batch['labels'].to(torch.long)\n",
        "\n",
        "            logits = lstm_model(input_ids, attention_mask)\n",
        "            predictions.extend(torch.argmax(logits, axis=1).tolist())\n",
        "            true_labels.extend(labels.tolist())\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, zero_division=0)\n",
        "    recall = recall_score(true_labels, predictions, zero_division=0)\n",
        "    f1 = f1_score(true_labels, predictions, zero_division=0)\n",
        "\n",
        "    epoch_metrics['precision'].append(precision)\n",
        "    epoch_metrics['recall'].append(recall)\n",
        "    epoch_metrics['f1'].append(f1)\n",
        "\n",
        "    print(f'Epoch {epoch} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}')\n",
        "\n",
        "# Plot confusion matrix\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Line chart for each metric over epochs\n",
        "plt.figure(figsize=(10, 5))\n",
        "for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
        "    plt.plot(range(1, 4), epoch_metrics[metric], label=metric.capitalize())\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Metric Scores Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Bar chart summarizing overall performance\n",
        "overall_metrics = {metric: sum(epoch_metrics[metric]) / len(epoch_metrics[metric]) for metric in epoch_metrics}\n",
        "plt.bar(overall_metrics.keys(), overall_metrics.values(), color=['blue', 'green', 'red', 'purple', 'orange'])\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Values')\n",
        "plt.title('Overall Model Performance')\n",
        "plt.ylim([0, 1])\n",
        "plt.show()\n",
        "\n",
        "# Markdown table for performance metrics per epoch\n",
        "markdown_table = \"| Epoch | Loss | Accuracy | Precision | Recall | F1 |\\n\"\n",
        "markdown_table += \"|-------|------|----------|-----------|--------|----|\\n\"\n",
        "for i in range(3):\n",
        "    markdown_table += f\"| {i + 1} \"\n",
        "    markdown_table += \"|\".join(f\"{epoch_metrics[metric][i]:.4f}\" for metric in ['loss', 'accuracy', 'precision', 'recall', 'f1'])\n",
        "    markdown_table += \"|\\n\"\n",
        "\n",
        "# Last row for overall metrics\n",
        "markdown_table += \"| Overall \"\n",
        "markdown_table += \"|\".join(f\"{overall:.4f}\" for overall in overall_metrics.values())\n",
        "markdown_table += \"|\\n\"\n",
        "\n",
        "display(Markdown(markdown_table))"
      ],
      "metadata": {
        "id": "QcfDiNtkK3sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PhoBERT + CNN (Convolutional Neural Network)"
      ],
      "metadata": {
        "id": "qWHAwhRqOi6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import necessary libraries and modules\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoModel, AutoTokenizer, AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import Markdown\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n",
        "\n",
        "# Custom CNN model that sits on top of PhoBERT\n",
        "class PhoBertCNN(nn.Module):\n",
        "    def __init__(self, phobert_model, num_filters, filter_sizes, num_classes):\n",
        "        super().__init__()\n",
        "        self.phobert = phobert_model\n",
        "        hidden_size = phobert_model.config.hidden_size\n",
        "        self.convs = nn.ModuleList(\n",
        "            [nn.Conv1d(hidden_size, num_filters, fs) for fs in filter_sizes]\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(num_filters * len(filter_sizes), num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Don't compute gradient for the backbone PhoBERT model\n",
        "        with torch.no_grad():\n",
        "            outputs = self.phobert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # We take only the hidden states\n",
        "        x = outputs[0]  # (B, L, D) - Batch, Length, Hidden Size\n",
        "        x = x.permute(0, 2, 1)  # Switch to (B, D, L) for Conv1D\n",
        "        x = [F.relu(conv(x)) for conv in self.convs]  # Apply each convolution layer\n",
        "        x = [F.max_pool1d(c, c.shape[2]).squeeze(2) for c in x]  # Max pooling over time\n",
        "        x = torch.cat(x, 1)  # Concatenate the feature maps\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = self.fc(x)  # Final fully connected layer\n",
        "        return x\n",
        "\n",
        "# Load the PhoBERT model\n",
        "phobert = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
        "\n",
        "# Instantiate the custom classifier\n",
        "num_classes = 2  # The number of output labels\n",
        "num_filters = 100  # The number of convolutional filters\n",
        "filter_sizes = [2, 3, 4]  # The size of the convolutional kernels\n",
        "\n",
        "# Create the PhoBertCNN model\n",
        "model = PhoBertCNN(\n",
        "    phobert_model=phobert,\n",
        "    num_filters=num_filters,\n",
        "    filter_sizes=filter_sizes,\n",
        "    num_classes=num_classes\n",
        ")\n",
        "\n",
        "# Prepare the dataset and dataloader\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Load training and testing data\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/NLP/NTC_SV_train_1.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/NLP/NTC_SV_test_1.csv')\n",
        "\n",
        "# Prepare the datasets\n",
        "train_encodings = tokenizer(train_df['review'].tolist(), truncation=True, padding=True, max_length=256)\n",
        "test_encodings = tokenizer(test_df['review'].tolist(), truncation=True, padding=True, max_length=256)\n",
        "\n",
        "train_dataset = SentimentDataset(train_encodings, train_df['label'].tolist())\n",
        "test_dataset = SentimentDataset(test_encodings, test_df['label'].tolist())\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Prepare for training\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6)\n",
        "\n",
        "# Training and evaluation loop\n",
        "epoch_metrics = {\n",
        "    'loss': [],\n",
        "    'accuracy': [],\n",
        "    'precision': [],\n",
        "    'recall': [],\n",
        "    'f1': []\n",
        "}\n",
        "\n",
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(torch.long)\n",
        "        attention_mask = batch['attention_mask'].to(torch.long)\n",
        "        labels = batch['labels'].to(torch.long)\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    epoch_metrics['loss'].append(avg_loss)\n",
        "\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader):\n",
        "            input_ids = batch['input_ids'].to(torch.long)\n",
        "            attention_mask = batch['attention_mask'].to(torch.long)\n",
        "            labels = batch['labels'].to(torch.long)\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            predictions.extend(preds.tolist())\n",
        "            true_labels.extend(labels.tolist())\n",
        "\n",
        "    # Calculate and store the metrics after each epoch\n",
        "    acc = accuracy_score(true_labels, predictions)\n",
        "    prec = precision_score(true_labels, predictions, zero_division=0)\n",
        "    rec = recall_score(true_labels, predictions, zero_division=0)\n",
        "    f1 = f1_score(true_labels, predictions, zero_division=0)\n",
        "    # Record the metrics for this epoch\n",
        "    epoch_metrics['accuracy'].append(acc)\n",
        "    epoch_metrics['precision'].append(prec)\n",
        "    epoch_metrics['recall'].append(rec)\n",
        "    epoch_metrics['f1'].append(f1)\n",
        "\n",
        "    print(f\"Epoch {epoch} - Loss: {avg_loss:.4f}, Acc: {acc:.4f}, Prec: {prec:.4f}, Rec: {rec:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "# Plot metrics over epochs\n",
        "plt.figure(figsize=(10, 5))\n",
        "epochs = range(1, 4)  # Assuming you have 3 epochs\n",
        "\n",
        "for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
        "    if epoch_metrics[metric]:  # Only plot if there are values in the list\n",
        "        plt.plot(epochs, epoch_metrics[metric], label=metric.capitalize())\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Metric Scores Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Bar chart summarizing overall performance\n",
        "overall_metrics = {metric: sum(epoch_metrics[metric]) / len(epoch_metrics[metric]) for metric in epoch_metrics}\n",
        "plt.bar(overall_metrics.keys(), overall_metrics.values(), color=['blue', 'green', 'red', 'purple', 'orange'])\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Values')\n",
        "plt.title('Overall Model Performance')\n",
        "plt.ylim([0, 1])\n",
        "plt.show()\n",
        "\n",
        "# Markdown table for performance metrics per epoch\n",
        "markdown_table = \"| Epoch | Loss | Accuracy | Precision | Recall | F1 |\\n\"\n",
        "markdown_table += \"|-------|------|----------|-----------|--------|----|\\n\"\n",
        "for i in range(3):\n",
        "    markdown_table += f\"| {i + 1} \"\n",
        "    markdown_table += \"|\".join(f\"{epoch_metrics[metric][i]:.4f}\" for metric in ['loss', 'accuracy', 'precision', 'recall', 'f1'])\n",
        "    markdown_table += \"|\\n\"\n",
        "\n",
        "# Last row for overall metrics\n",
        "markdown_table += \"| Overall \"\n",
        "markdown_table += \"|\".join(f\"{overall:.4f}\" for overall in overall_metrics.values())\n",
        "markdown_table += \"|\\n\"\n",
        "\n",
        "display(Markdown(markdown_table))\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "TP = cm[1, 1]\n",
        "FP = cm[0, 1]\n",
        "TN = cm[0, 0]\n",
        "FN = cm[1, 0]\n",
        "\n",
        "print(f\"True Positives (TP): {TP}\")\n",
        "print(f\"False Positives (FP): {FP}\")\n",
        "print(f\"True Negatives (TN): {TN}\")\n",
        "print(f\"False Negatives (FN): {FN}\")\n",
        "\n",
        "# Plot the confusion matrix\n",
        "fig, ax = plt.subplots()\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
        "\n",
        "# Labels, title and ticks\n",
        "label_font = {'size':'16'}\n",
        "ax.set_xlabel('Predicted labels', fontdict=label_font)\n",
        "ax.set_ylabel('True labels', fontdict=label_font)\n",
        "ax.set_title('Data Preview', fontdict=label_font)\n",
        "ax.tick_params(axis='both', which='major', labelsize=14)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-fBkoMePOipn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}