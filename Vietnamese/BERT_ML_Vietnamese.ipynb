{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffc18c44-220b-414e-ba9b-cfe513656c2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr  5 14:14:16 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1080        On  | 00000000:04:00.0 Off |                  N/A |\n",
      "|  0%   29C    P8               7W / 180W |    330MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce GTX 1080        On  | 00000000:05:00.0 Off |                  N/A |\n",
      "|  0%   29C    P8               7W / 180W |    330MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 2080 Ti     On  | 00000000:08:00.0 Off |                  N/A |\n",
      "|  0%   38C    P2              61W / 250W |  10825MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 2080 Ti     On  | 00000000:09:00.0 Off |                  N/A |\n",
      "|  0%   27C    P8               1W / 250W |    471MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce GTX 1080        On  | 00000000:83:00.0 Off |                  N/A |\n",
      "|  0%   28C    P8               7W / 180W |    330MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce GTX 1080        On  | 00000000:84:00.0 Off |                  N/A |\n",
      "|  0%   28C    P8               6W / 180W |    330MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA GeForce RTX 2080 Ti     On  | 00000000:87:00.0 Off |                  N/A |\n",
      "|  0%   27C    P8              18W / 250W |    471MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   1856934      C   /usr/bin/python3                            220MiB |\n",
      "|    0   N/A  N/A   1878437      C   /usr/bin/python3                            106MiB |\n",
      "|    1   N/A  N/A   1856934      C   /usr/bin/python3                            220MiB |\n",
      "|    1   N/A  N/A   1878437      C   /usr/bin/python3                            106MiB |\n",
      "|    2   N/A  N/A   1856934      C   /usr/bin/python3                           9994MiB |\n",
      "|    2   N/A  N/A   1878437      C   /usr/bin/python3                            828MiB |\n",
      "|    3   N/A  N/A   1856934      C   /usr/bin/python3                            312MiB |\n",
      "|    3   N/A  N/A   1878437      C   /usr/bin/python3                            156MiB |\n",
      "|    4   N/A  N/A   1856934      C   /usr/bin/python3                            220MiB |\n",
      "|    4   N/A  N/A   1878437      C   /usr/bin/python3                            106MiB |\n",
      "|    5   N/A  N/A   1856934      C   /usr/bin/python3                            220MiB |\n",
      "|    5   N/A  N/A   1878437      C   /usr/bin/python3                            106MiB |\n",
      "|    6   N/A  N/A   1856934      C   /usr/bin/python3                            312MiB |\n",
      "|    6   N/A  N/A   1878437      C   /usr/bin/python3                            156MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "368b3243-8252-4a8e-a5cd-0312cc273bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the CUDA_VISIBLE_DEVICES environment variable to the index of the GPU you want to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"  # Set to the index of the GPU you want to use, e.g., \"0\" for the first GPU\n",
    "\n",
    "# Now, when you run your code, it will only see the GPU with the specified index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "692e596b-81fc-44f5-9850-ee8fe659f8c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hygumm/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-05 14:15:39.308300: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-05 14:15:39.589165: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-05 14:15:40.617848: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-05 14:15:40.617889: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f59dc51-7513-4678-95e9-0a34596b778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8516f13d-0b3c-4c26-9a3a-7010e8d90392",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "df1 = pd.read_csv(\"/home/hygumm/NLP_Project/NTC_SV_train.csv\")\n",
    "df2 = pd.read_csv(\"/home/hygumm/NLP_Project/NTC_SV_test.csv\")\n",
    "df= pd.concat([df1,df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4688da5-f4a3-4d91-ae70-395975338814",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a14e92d-c72a-4f11-9e3c-1d7d669b61a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'review':'text'}, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1394ea74-ae51-4968-8e19-f281889957e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>đồ_ăn ngon positive hợp_khẩu vị nhiều món nhân...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chè bơ thơm positive có vị ngậy ngậy nhưng lại...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chiều hôm nay mới đi ăn về nghe thiên_hạ đồn q...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mình đặt_hàng qua tin nhắn với cửa_hàng hứa sá...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ghé mấy lần rồi mà không review đi đâu cũng ch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  đồ_ăn ngon positive hợp_khẩu vị nhiều món nhân...      0\n",
       "1  chè bơ thơm positive có vị ngậy ngậy nhưng lại...      0\n",
       "2  chiều hôm nay mới đi ăn về nghe thiên_hạ đồn q...      0\n",
       "3  mình đặt_hàng qua tin nhắn với cửa_hàng hứa sá...      0\n",
       "4  ghé mấy lần rồi mà không review đi đâu cũng ch...      1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82d85b2b-dd57-49c6-b54a-6e453398cc01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50760"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecb09d7d-2e09-4b2f-bc5d-a5dcfcbcc3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd686c0a-48a3-454f-b765-f4c19c0d10c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.sample(n=1000, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6169d3c5-d446-42be-a6aa-90fedf153ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.3)  # 30% for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dd2eab3-c8c6-4c76-9110-b3ef6d47b444",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6928a5f2-5c55-453c-91cb-031322212310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>đồ_ăn ngon positive hợp_khẩu vị nhiều món nhân...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chè bơ thơm positive có vị ngậy ngậy nhưng lại...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chiều hôm nay mới đi ăn về nghe thiên_hạ đồn q...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mình đặt_hàng qua tin nhắn với cửa_hàng hứa sá...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ghé mấy lần rồi mà không review đi đâu cũng ch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  đồ_ăn ngon positive hợp_khẩu vị nhiều món nhân...      0\n",
       "1  chè bơ thơm positive có vị ngậy ngậy nhưng lại...      0\n",
       "2  chiều hôm nay mới đi ăn về nghe thiên_hạ đồn q...      0\n",
       "3  mình đặt_hàng qua tin nhắn với cửa_hàng hứa sá...      0\n",
       "4  ghé mấy lần rồi mà không review đi đâu cũng ch...      1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8413b7a-3563-44c9-8968-7d65b25a2733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, tokenizer, max_len=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # Drop rows with NaN values\n",
    "    df = df.dropna()\n",
    "\n",
    "    for text in df['text']:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(df['label'].values)\n",
    "\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "# Preprocess both the training and testing set\n",
    "train_inputs, train_masks, train_labels = preprocess_data(train_df, tokenizer)\n",
    "test_inputs, test_masks, test_labels = preprocess_data(val_df, tokenizer)  # Assuming val_df is your test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "966e9996-8a77-4651-bb9a-11a8b73bb2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16  # Adjust based on your hardware capabilities\n",
    "\n",
    "# Create the DataLoader for the training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for the test set\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf969102-aeb8-4e5b-86da-eafc0f46436d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-multilingual-cased',\n",
    "    num_labels=2,  # Binary classification\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "# Specify the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a224ed6-5699-4b23-baf4-6610634ce5f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hygumm/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=2e-5,  # Learning rate\n",
    "                  eps=1e-8)  # Epsilon to prevent division by zero in optimizer\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is the number of batches * number of epochs\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0,  # Default value\n",
    "                                            num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11d1ec85-d292-4fb9-8eae-97ff7f5ec680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f3a771a-7ed3-4c3f-89d6-680961364433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  2,221.    Elapsed: 0:00:15.\n",
      "  Batch    80  of  2,221.    Elapsed: 0:00:27.\n",
      "  Batch   120  of  2,221.    Elapsed: 0:00:39.\n",
      "  Batch   160  of  2,221.    Elapsed: 0:00:52.\n",
      "  Batch   200  of  2,221.    Elapsed: 0:01:04.\n",
      "  Batch   240  of  2,221.    Elapsed: 0:01:16.\n",
      "  Batch   280  of  2,221.    Elapsed: 0:01:28.\n",
      "  Batch   320  of  2,221.    Elapsed: 0:01:41.\n",
      "  Batch   360  of  2,221.    Elapsed: 0:01:53.\n",
      "  Batch   400  of  2,221.    Elapsed: 0:02:05.\n",
      "  Batch   440  of  2,221.    Elapsed: 0:02:18.\n",
      "  Batch   480  of  2,221.    Elapsed: 0:02:30.\n",
      "  Batch   520  of  2,221.    Elapsed: 0:02:42.\n",
      "  Batch   560  of  2,221.    Elapsed: 0:02:55.\n",
      "  Batch   600  of  2,221.    Elapsed: 0:03:07.\n",
      "  Batch   640  of  2,221.    Elapsed: 0:03:19.\n",
      "  Batch   680  of  2,221.    Elapsed: 0:03:32.\n",
      "  Batch   720  of  2,221.    Elapsed: 0:03:44.\n",
      "  Batch   760  of  2,221.    Elapsed: 0:03:57.\n",
      "  Batch   800  of  2,221.    Elapsed: 0:04:09.\n",
      "  Batch   840  of  2,221.    Elapsed: 0:04:21.\n",
      "  Batch   880  of  2,221.    Elapsed: 0:04:34.\n",
      "  Batch   920  of  2,221.    Elapsed: 0:04:46.\n",
      "  Batch   960  of  2,221.    Elapsed: 0:04:58.\n",
      "  Batch 1,000  of  2,221.    Elapsed: 0:05:11.\n",
      "  Batch 1,040  of  2,221.    Elapsed: 0:05:23.\n",
      "  Batch 1,080  of  2,221.    Elapsed: 0:05:35.\n",
      "  Batch 1,120  of  2,221.    Elapsed: 0:05:48.\n",
      "  Batch 1,160  of  2,221.    Elapsed: 0:06:00.\n",
      "  Batch 1,200  of  2,221.    Elapsed: 0:06:13.\n",
      "  Batch 1,240  of  2,221.    Elapsed: 0:06:25.\n",
      "  Batch 1,280  of  2,221.    Elapsed: 0:06:37.\n",
      "  Batch 1,320  of  2,221.    Elapsed: 0:06:50.\n",
      "  Batch 1,360  of  2,221.    Elapsed: 0:07:02.\n",
      "  Batch 1,400  of  2,221.    Elapsed: 0:07:14.\n",
      "  Batch 1,440  of  2,221.    Elapsed: 0:07:27.\n",
      "  Batch 1,480  of  2,221.    Elapsed: 0:07:39.\n",
      "  Batch 1,520  of  2,221.    Elapsed: 0:07:52.\n",
      "  Batch 1,560  of  2,221.    Elapsed: 0:08:04.\n",
      "  Batch 1,600  of  2,221.    Elapsed: 0:08:16.\n",
      "  Batch 1,640  of  2,221.    Elapsed: 0:08:29.\n",
      "  Batch 1,680  of  2,221.    Elapsed: 0:08:41.\n",
      "  Batch 1,720  of  2,221.    Elapsed: 0:08:53.\n",
      "  Batch 1,760  of  2,221.    Elapsed: 0:09:06.\n",
      "  Batch 1,800  of  2,221.    Elapsed: 0:09:18.\n",
      "  Batch 1,840  of  2,221.    Elapsed: 0:09:30.\n",
      "  Batch 1,880  of  2,221.    Elapsed: 0:09:43.\n",
      "  Batch 1,920  of  2,221.    Elapsed: 0:09:55.\n",
      "  Batch 1,960  of  2,221.    Elapsed: 0:10:08.\n",
      "  Batch 2,000  of  2,221.    Elapsed: 0:10:20.\n",
      "  Batch 2,040  of  2,221.    Elapsed: 0:10:32.\n",
      "  Batch 2,080  of  2,221.    Elapsed: 0:10:45.\n",
      "  Batch 2,120  of  2,221.    Elapsed: 0:10:57.\n",
      "  Batch 2,160  of  2,221.    Elapsed: 0:11:09.\n",
      "  Batch 2,200  of  2,221.    Elapsed: 0:11:22.\n",
      "\n",
      "  Average training loss: 0.35\n",
      "  Training epoch took: 0:11:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.89\n",
      "  Validation Loss: 0.28\n",
      "  Validation took: 0:01:41\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  2,221.    Elapsed: 0:00:12.\n",
      "  Batch    80  of  2,221.    Elapsed: 0:00:25.\n",
      "  Batch   120  of  2,221.    Elapsed: 0:00:37.\n",
      "  Batch   160  of  2,221.    Elapsed: 0:00:49.\n",
      "  Batch   200  of  2,221.    Elapsed: 0:01:02.\n",
      "  Batch   240  of  2,221.    Elapsed: 0:01:14.\n",
      "  Batch   280  of  2,221.    Elapsed: 0:01:27.\n",
      "  Batch   320  of  2,221.    Elapsed: 0:01:39.\n",
      "  Batch   360  of  2,221.    Elapsed: 0:01:51.\n",
      "  Batch   400  of  2,221.    Elapsed: 0:02:04.\n",
      "  Batch   440  of  2,221.    Elapsed: 0:02:16.\n",
      "  Batch   480  of  2,221.    Elapsed: 0:02:28.\n",
      "  Batch   520  of  2,221.    Elapsed: 0:02:41.\n",
      "  Batch   560  of  2,221.    Elapsed: 0:02:53.\n",
      "  Batch   600  of  2,221.    Elapsed: 0:03:06.\n",
      "  Batch   640  of  2,221.    Elapsed: 0:03:18.\n",
      "  Batch   680  of  2,221.    Elapsed: 0:03:30.\n",
      "  Batch   720  of  2,221.    Elapsed: 0:03:43.\n",
      "  Batch   760  of  2,221.    Elapsed: 0:03:55.\n",
      "  Batch   800  of  2,221.    Elapsed: 0:04:07.\n",
      "  Batch   840  of  2,221.    Elapsed: 0:04:20.\n",
      "  Batch   880  of  2,221.    Elapsed: 0:04:32.\n",
      "  Batch   920  of  2,221.    Elapsed: 0:04:45.\n",
      "  Batch   960  of  2,221.    Elapsed: 0:04:57.\n",
      "  Batch 1,000  of  2,221.    Elapsed: 0:05:09.\n",
      "  Batch 1,040  of  2,221.    Elapsed: 0:05:22.\n",
      "  Batch 1,080  of  2,221.    Elapsed: 0:05:34.\n",
      "  Batch 1,120  of  2,221.    Elapsed: 0:05:46.\n",
      "  Batch 1,160  of  2,221.    Elapsed: 0:05:59.\n",
      "  Batch 1,200  of  2,221.    Elapsed: 0:06:11.\n",
      "  Batch 1,240  of  2,221.    Elapsed: 0:06:24.\n",
      "  Batch 1,280  of  2,221.    Elapsed: 0:06:36.\n",
      "  Batch 1,320  of  2,221.    Elapsed: 0:06:48.\n",
      "  Batch 1,360  of  2,221.    Elapsed: 0:07:01.\n",
      "  Batch 1,400  of  2,221.    Elapsed: 0:07:13.\n",
      "  Batch 1,440  of  2,221.    Elapsed: 0:07:25.\n",
      "  Batch 1,480  of  2,221.    Elapsed: 0:07:38.\n",
      "  Batch 1,520  of  2,221.    Elapsed: 0:07:50.\n",
      "  Batch 1,560  of  2,221.    Elapsed: 0:08:03.\n",
      "  Batch 1,600  of  2,221.    Elapsed: 0:08:15.\n",
      "  Batch 1,640  of  2,221.    Elapsed: 0:08:27.\n",
      "  Batch 1,680  of  2,221.    Elapsed: 0:08:40.\n",
      "  Batch 1,720  of  2,221.    Elapsed: 0:08:52.\n",
      "  Batch 1,760  of  2,221.    Elapsed: 0:09:04.\n",
      "  Batch 1,800  of  2,221.    Elapsed: 0:09:17.\n",
      "  Batch 1,840  of  2,221.    Elapsed: 0:09:29.\n",
      "  Batch 1,880  of  2,221.    Elapsed: 0:09:42.\n",
      "  Batch 1,920  of  2,221.    Elapsed: 0:09:54.\n",
      "  Batch 1,960  of  2,221.    Elapsed: 0:10:06.\n",
      "  Batch 2,000  of  2,221.    Elapsed: 0:10:19.\n",
      "  Batch 2,040  of  2,221.    Elapsed: 0:10:31.\n",
      "  Batch 2,080  of  2,221.    Elapsed: 0:10:44.\n",
      "  Batch 2,120  of  2,221.    Elapsed: 0:10:56.\n",
      "  Batch 2,160  of  2,221.    Elapsed: 0:11:08.\n",
      "  Batch 2,200  of  2,221.    Elapsed: 0:11:21.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoch took: 0:11:27\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.89\n",
      "  Validation Loss: 0.28\n",
      "  Validation took: 0:01:41\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  2,221.    Elapsed: 0:00:12.\n",
      "  Batch    80  of  2,221.    Elapsed: 0:00:25.\n",
      "  Batch   120  of  2,221.    Elapsed: 0:00:37.\n",
      "  Batch   160  of  2,221.    Elapsed: 0:00:50.\n",
      "  Batch   200  of  2,221.    Elapsed: 0:01:02.\n",
      "  Batch   240  of  2,221.    Elapsed: 0:01:14.\n",
      "  Batch   280  of  2,221.    Elapsed: 0:01:27.\n",
      "  Batch   320  of  2,221.    Elapsed: 0:01:39.\n",
      "  Batch   360  of  2,221.    Elapsed: 0:01:52.\n",
      "  Batch   400  of  2,221.    Elapsed: 0:02:04.\n",
      "  Batch   440  of  2,221.    Elapsed: 0:02:16.\n",
      "  Batch   480  of  2,221.    Elapsed: 0:02:29.\n",
      "  Batch   520  of  2,221.    Elapsed: 0:02:41.\n",
      "  Batch   560  of  2,221.    Elapsed: 0:02:53.\n",
      "  Batch   600  of  2,221.    Elapsed: 0:03:06.\n",
      "  Batch   640  of  2,221.    Elapsed: 0:03:18.\n",
      "  Batch   680  of  2,221.    Elapsed: 0:03:31.\n",
      "  Batch   720  of  2,221.    Elapsed: 0:03:43.\n",
      "  Batch   760  of  2,221.    Elapsed: 0:03:55.\n",
      "  Batch   800  of  2,221.    Elapsed: 0:04:08.\n",
      "  Batch   840  of  2,221.    Elapsed: 0:04:20.\n",
      "  Batch   880  of  2,221.    Elapsed: 0:04:33.\n",
      "  Batch   920  of  2,221.    Elapsed: 0:04:45.\n",
      "  Batch   960  of  2,221.    Elapsed: 0:04:57.\n",
      "  Batch 1,000  of  2,221.    Elapsed: 0:05:10.\n",
      "  Batch 1,040  of  2,221.    Elapsed: 0:05:22.\n",
      "  Batch 1,080  of  2,221.    Elapsed: 0:05:34.\n",
      "  Batch 1,120  of  2,221.    Elapsed: 0:05:47.\n",
      "  Batch 1,160  of  2,221.    Elapsed: 0:05:59.\n",
      "  Batch 1,200  of  2,221.    Elapsed: 0:06:12.\n",
      "  Batch 1,240  of  2,221.    Elapsed: 0:06:24.\n",
      "  Batch 1,280  of  2,221.    Elapsed: 0:06:36.\n",
      "  Batch 1,320  of  2,221.    Elapsed: 0:06:49.\n",
      "  Batch 1,360  of  2,221.    Elapsed: 0:07:01.\n",
      "  Batch 1,400  of  2,221.    Elapsed: 0:07:14.\n",
      "  Batch 1,440  of  2,221.    Elapsed: 0:07:26.\n",
      "  Batch 1,480  of  2,221.    Elapsed: 0:07:38.\n",
      "  Batch 1,520  of  2,221.    Elapsed: 0:07:51.\n",
      "  Batch 1,560  of  2,221.    Elapsed: 0:08:03.\n",
      "  Batch 1,600  of  2,221.    Elapsed: 0:08:15.\n",
      "  Batch 1,640  of  2,221.    Elapsed: 0:08:28.\n",
      "  Batch 1,680  of  2,221.    Elapsed: 0:08:40.\n",
      "  Batch 1,720  of  2,221.    Elapsed: 0:08:53.\n",
      "  Batch 1,760  of  2,221.    Elapsed: 0:09:05.\n",
      "  Batch 1,800  of  2,221.    Elapsed: 0:09:17.\n",
      "  Batch 1,840  of  2,221.    Elapsed: 0:09:30.\n",
      "  Batch 1,880  of  2,221.    Elapsed: 0:09:42.\n",
      "  Batch 1,920  of  2,221.    Elapsed: 0:09:55.\n",
      "  Batch 1,960  of  2,221.    Elapsed: 0:10:07.\n",
      "  Batch 2,000  of  2,221.    Elapsed: 0:10:19.\n",
      "  Batch 2,040  of  2,221.    Elapsed: 0:10:32.\n",
      "  Batch 2,080  of  2,221.    Elapsed: 0:10:44.\n",
      "  Batch 2,120  of  2,221.    Elapsed: 0:10:56.\n",
      "  Batch 2,160  of  2,221.    Elapsed: 0:11:09.\n",
      "  Batch 2,200  of  2,221.    Elapsed: 0:11:21.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:11:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.89\n",
      "  Validation Loss: 0.34\n",
      "  Validation took: 0:01:41\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  2,221.    Elapsed: 0:00:12.\n",
      "  Batch    80  of  2,221.    Elapsed: 0:00:25.\n",
      "  Batch   120  of  2,221.    Elapsed: 0:00:37.\n",
      "  Batch   160  of  2,221.    Elapsed: 0:00:50.\n",
      "  Batch   200  of  2,221.    Elapsed: 0:01:02.\n",
      "  Batch   240  of  2,221.    Elapsed: 0:01:14.\n",
      "  Batch   280  of  2,221.    Elapsed: 0:01:27.\n",
      "  Batch   320  of  2,221.    Elapsed: 0:01:39.\n",
      "  Batch   360  of  2,221.    Elapsed: 0:01:51.\n",
      "  Batch   400  of  2,221.    Elapsed: 0:02:04.\n",
      "  Batch   440  of  2,221.    Elapsed: 0:02:16.\n",
      "  Batch   480  of  2,221.    Elapsed: 0:02:28.\n",
      "  Batch   520  of  2,221.    Elapsed: 0:02:41.\n",
      "  Batch   560  of  2,221.    Elapsed: 0:02:53.\n",
      "  Batch   600  of  2,221.    Elapsed: 0:03:06.\n",
      "  Batch   640  of  2,221.    Elapsed: 0:03:18.\n",
      "  Batch   680  of  2,221.    Elapsed: 0:03:30.\n",
      "  Batch   720  of  2,221.    Elapsed: 0:03:43.\n",
      "  Batch   760  of  2,221.    Elapsed: 0:03:55.\n",
      "  Batch   800  of  2,221.    Elapsed: 0:04:08.\n",
      "  Batch   840  of  2,221.    Elapsed: 0:04:20.\n",
      "  Batch   880  of  2,221.    Elapsed: 0:04:32.\n",
      "  Batch   920  of  2,221.    Elapsed: 0:04:45.\n",
      "  Batch   960  of  2,221.    Elapsed: 0:04:57.\n",
      "  Batch 1,000  of  2,221.    Elapsed: 0:05:10.\n",
      "  Batch 1,040  of  2,221.    Elapsed: 0:05:22.\n",
      "  Batch 1,080  of  2,221.    Elapsed: 0:05:34.\n",
      "  Batch 1,120  of  2,221.    Elapsed: 0:05:47.\n",
      "  Batch 1,160  of  2,221.    Elapsed: 0:05:59.\n",
      "  Batch 1,200  of  2,221.    Elapsed: 0:06:11.\n",
      "  Batch 1,240  of  2,221.    Elapsed: 0:06:24.\n",
      "  Batch 1,280  of  2,221.    Elapsed: 0:06:36.\n",
      "  Batch 1,320  of  2,221.    Elapsed: 0:06:49.\n",
      "  Batch 1,360  of  2,221.    Elapsed: 0:07:01.\n",
      "  Batch 1,400  of  2,221.    Elapsed: 0:07:13.\n",
      "  Batch 1,440  of  2,221.    Elapsed: 0:07:26.\n",
      "  Batch 1,480  of  2,221.    Elapsed: 0:07:38.\n",
      "  Batch 1,520  of  2,221.    Elapsed: 0:07:51.\n",
      "  Batch 1,560  of  2,221.    Elapsed: 0:08:03.\n",
      "  Batch 1,600  of  2,221.    Elapsed: 0:08:15.\n",
      "  Batch 1,640  of  2,221.    Elapsed: 0:08:28.\n",
      "  Batch 1,680  of  2,221.    Elapsed: 0:08:40.\n",
      "  Batch 1,720  of  2,221.    Elapsed: 0:08:53.\n",
      "  Batch 1,760  of  2,221.    Elapsed: 0:09:05.\n",
      "  Batch 1,800  of  2,221.    Elapsed: 0:09:17.\n",
      "  Batch 1,840  of  2,221.    Elapsed: 0:09:30.\n",
      "  Batch 1,880  of  2,221.    Elapsed: 0:09:42.\n",
      "  Batch 1,920  of  2,221.    Elapsed: 0:09:54.\n",
      "  Batch 1,960  of  2,221.    Elapsed: 0:10:07.\n",
      "  Batch 2,000  of  2,221.    Elapsed: 0:10:19.\n",
      "  Batch 2,040  of  2,221.    Elapsed: 0:10:32.\n",
      "  Batch 2,080  of  2,221.    Elapsed: 0:10:44.\n",
      "  Batch 2,120  of  2,221.    Elapsed: 0:10:56.\n",
      "  Batch 2,160  of  2,221.    Elapsed: 0:11:09.\n",
      "  Batch 2,200  of  2,221.    Elapsed: 0:11:21.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:11:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.89\n",
      "  Validation Loss: 0.37\n",
      "  Validation took: 0:01:41\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to calculate the accuracy of predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# Function to format elapsed times as hh:mm:ss\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round(elapsed))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "# Training loop\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progress update every 40 batches\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a backward pass\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch)\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Accumulate the training loss over all of the batches\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(\"\\n  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on our validation set\n",
    "\n",
    "    print(\"\\nRunning Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "            \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(test_dataloader)\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97fc4f33-6369-4ddd-8f4c-1adfd7b99d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[6668  950]\n",
      " [ 676 6934]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Initialize lists to store true labels and predicted labels\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "# Evaluate model on the validation set and collect true and predicted labels\n",
    "for batch in test_dataloader:\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "\n",
    "    with torch.no_grad():        \n",
    "        outputs = model(b_input_ids,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=b_input_mask,\n",
    "                        labels=b_labels)\n",
    "    \n",
    "    logits = outputs.logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    pred_labels.extend(np.argmax(logits, axis=1).flatten())\n",
    "    true_labels.extend(label_ids.flatten())\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1ada752-335b-4e8d-b4ec-0b33d81dcfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment Label: 1\n",
      "Probabilities: tensor([[0.0166, 0.9834]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Function to predict sentiment label of a sentence\n",
    "def predict_sentiment(sentence):\n",
    "    # Tokenize the input sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Move inputs to the appropriate device\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    \n",
    "    # Perform inference (prediction)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = softmax(outputs.logits, dim=1)\n",
    "    \n",
    "    # Get the predicted label (0 or 1)\n",
    "    predicted_label = torch.argmax(probabilities, dim=1).item()\n",
    "    \n",
    "    return predicted_label, probabilities\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"I love this movie!\"\n",
    "\n",
    "# Predict sentiment label\n",
    "predicted_label, probabilities = predict_sentiment(sentence)\n",
    "print(\"Predicted Sentiment Label:\", predicted_label)\n",
    "print(\"Probabilities:\", probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bed8a609-792d-4fcb-a738-c87f4689092e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment Label: 1\n",
      "Probabilities: tensor([[0.0443, 0.9557]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Example sentence in Vietnamese\n",
    "example_sentence = \"Món ăn ngon và phục vụ tốt!\"\n",
    "#Delicious food and good service!\n",
    "# Predict sentiment label\n",
    "predicted_label, probabilities = predict_sentiment(example_sentence)\n",
    "print(\"Predicted Sentiment Label:\", predicted_label)\n",
    "print(\"Probabilities:\", probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77a1cbac-4308-435f-b4e0-e2a3e89f1bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence =  \"Tôi ghét đồ ăn tôi không thích, tôi sắp nôn mất rồi\"\n",
    "#I hate the food I didnt like it i was about to vomit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3dbc8a35-db38-4bfc-8cd4-460923284a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment Label: 0\n",
      "Probabilities: tensor([[0.9985, 0.0015]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Predict sentiment label\n",
    "predicted_label, probabilities = predict_sentiment(example_sentence)\n",
    "print(\"Predicted Sentiment Label:\", predicted_label)\n",
    "print(\"Probabilities:\", probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db7261f9-5868-4096-a129-b57643ea9225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment Label: 0\n",
      "Probabilities: tensor([[0.9970, 0.0030]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Example sentence\n",
    "sentence = '''Nothing new....mrunal bad choice....few comedy scenes.....very overacting scenes....ott film not a theatre one .....Vijay overacting spoiled ...\n",
    "Story is uncooked....funny fights for six feet guy.....\n",
    "one scene I must tell jagapathi babu first intro ....not even ten seconds \n",
    "he give another chance to Vijay for big role with big salary even though \n",
    "he rejected his offer very filmy comedy scenes only happens in movie ....very boring second half ......mrunal wake up.....'''\n",
    "\n",
    "# Predict sentiment label\n",
    "predicted_label, probabilities = predict_sentiment(sentence)\n",
    "print(\"Predicted Sentiment Label:\", predicted_label)\n",
    "print(\"Probabilities:\", probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea2fe20-9738-403e-a8b4-0f6b2d63ecb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
